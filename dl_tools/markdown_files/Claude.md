---
organization: Anthropic
use_cases:
  - Claude is commonly used for writing, tutoring, summarizing, and ideation.
  - Available via Claude for Work and the Anthropic API, often used in customer service and enterprise workflows.
  - Claude is positioned as a safer alternative to traditional LLMs, emphasizing responsible use.
practices:
  - Claude does not use your inputs or outputs for model training unless you opt in (e.g., by joining a trusted tester program).
  - If you submit feedback (e.g., via thumbs up/down), the full conversation is stored for up to 10 years and may be used to improve or train models.
  - Inputs/outputs that violate the usage policy are stored for up to 2 years, and classification scores for up to 7 years.
  - Feedback data is de-identified (not linked to your user ID) before being used for research or model improvement.
  - Conversations you delete are removed from history and backend servers within 30 days.
data_info:
  - Anthropic has not released detailed information about the data used to train Claude.
  - No indication of training on user data from the general public unless explicitly allowed.
concerning_practices:
  - Trust & safety flagged conversations can be retained for up to 7 years; feedback is stored for 10 years and may be used in model training.
  - Anthropic has not publicly detailed what data Claude was initially trained on, raising transparency concerns.
  - Once flagged by the system, there is no way for users to request removal of such data from internal models.
concerning_practices_urls:
  - https://privacy.anthropic.com/en/articles/10023548-how-long-do-you-store-my-data
  - https://privacy.anthropic.com/en/articles/10023580-is-my-data-used-for-model-training
severity: medium
---

# Claude Details
Anthropicâ€™s AI assistant focused on helpfulness, honesty, and harmlessness, designed with safety and long-term alignment in mind.
